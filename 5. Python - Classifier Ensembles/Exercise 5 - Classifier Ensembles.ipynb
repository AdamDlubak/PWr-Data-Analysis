{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_path):\n",
    "    data = pd.read_csv(input_path)\n",
    "    data = shuffle(data, random_state = 102)\n",
    "    return data\n",
    "\n",
    "def import_dataset(id):\n",
    "    if id == 1:\n",
    "        input_dataset = \"Datasets/iris.csv\"\n",
    "    elif id == 2:\n",
    "        input_dataset = \"Datasets/glass.csv\"\n",
    "    elif id == 3:\n",
    "        input_dataset = \"Datasets/wine.csv\"\n",
    "    else:\n",
    "        input_dataset = \"Datasets/pima-indians-diabetes.csv\"\n",
    "\n",
    "    return load_dataset(input_dataset)   \n",
    "\n",
    "def split_data_on_x_y(dataset):\n",
    "    X = np.split(dataset, [-1], axis=1)\n",
    "    y = X[1]\n",
    "    X = X[0]\n",
    "    return X, y\n",
    "\n",
    "def split_data_on_training_test(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=142)\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def do_normalization(X, X_train, X_test):\n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X)\n",
    "\n",
    "    X = scaler.transform(X)\n",
    "    X_train = scaler.transform(X_train)  \n",
    "    X_test = scaler.transform(X_test)  \n",
    "    return X, X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_validation(y1, y2, folds_number):\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    ax.set_ylabel('F1-Score')\n",
    "    ax.set_title('K-Fold vs Stratified K-Fold')\n",
    "\n",
    "    x = np.arange(len(folds_number))\n",
    "\n",
    "    width = 0.25\n",
    "    ax.bar(x, y1, width, label='K-Fold')\n",
    "    ax.bar(x + width, y2, width,\n",
    "            color=list(plt.rcParams['axes.prop_cycle'])[2]['color'], label='Stratified K-Fold')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(folds_number)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_f1_macro_mean_score(scores):\n",
    "    for key, score in scores.items():\n",
    "        if key == 'test_f1_macro':\n",
    "            return np.mean(score)\n",
    "        \n",
    "def cross_validation(data, target, cv, clf):\n",
    "    clf = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    scores = cross_validate(clf, data, target, cv=cv, scoring=scoring, return_train_score=False)\n",
    "    f1_macro = return_f1_macro_mean_score(scores)\n",
    "    return f1_macro\n",
    "    \n",
    "def do_cross_validation_test(X, y, X_test, y_test, classifier, folds_number = [2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "    f1_kfold_score = []\n",
    "    f1_stratified_kfold_score = []\n",
    "\n",
    "    f1_kfold_score = []\n",
    "    f1_stratified_kfold_score = []\n",
    "\n",
    "    for i in folds_number:\n",
    "        \n",
    "        f1_kfold = cross_validation(X, np.ravel(y), KFold(n_splits=i), classifier)\n",
    "        f1_kfold_score.append(f1_kfold)\n",
    "        \n",
    "        f1_stratified_kfold = cross_validation(X, np.ravel(y), StratifiedKFold(n_splits=i), classifier)\n",
    "        f1_stratified_kfold_score.append(f1_stratified_kfold) \n",
    "        \n",
    "        print(\"K = {}\\t{:.3f}\\t{:.3f}\".format(i, f1_kfold, f1_stratified_kfold))\n",
    "        \n",
    "    plot_cross_validation(f1_kfold_score, f1_stratified_kfold_score, folds_number)\n",
    "\n",
    "def plot_confusion_matrix(Y_test, Y_pred):\n",
    "    cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "    img = plt.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.colorbar(img, fraction=0.045)\n",
    "    plt.grid('off')\n",
    "    for x in range(cm.shape[0]):\n",
    "        for y in range(cm.shape[1]):\n",
    "            plt.text(x, y, \"%0.2f\" % cm[x,y], \n",
    "                     size=12,  ha=\"center\", va=\"center\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "def cross_validation(data, target, cv, clf):\n",
    "    \n",
    "    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    scores = cross_validate(clf, data, target, cv=cv, scoring=scoring, return_train_score=False)\n",
    "    f1_macro = return_f1_macro_mean_score(scores)\n",
    "    return f1_macro\n",
    "    \n",
    "    \n",
    "def do_bagging_test(X, y, X_test, n_fold, params):\n",
    "    for param in params:\n",
    "        clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                            **param\n",
    "                           )\n",
    "        f1_score = cross_validation(X, np.ravel(y), StratifiedKFold(n_splits=n_fold), clf)\n",
    "        for key, value in param.items():\n",
    "            print(\"\\t{}\\t{}\\t{:.3f}\".format(key, value, f1_score))           \n",
    "\n",
    "def do_boosting_test(X, y, X_test, n_fold, params):\n",
    "    for param in params:\n",
    "        clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                            **param\n",
    "                           )\n",
    "        f1_score = cross_validation(X, np.ravel(y), StratifiedKFold(n_splits=n_fold), clf)\n",
    "        for key, value in param.items():\n",
    "            print(\"\\t{}\\t{}\\t{:.3f}\".format(key, value, f1_score))           \n",
    "\n",
    "def do_random_forest_test(X, y, X_test, n_fold, params):\n",
    "    for param in params:\n",
    "        clf = RandomForestClassifier(**param)\n",
    "        f1_score = cross_validation(X, np.ravel(y), StratifiedKFold(n_splits=n_fold), clf)\n",
    "        for key, value in param.items():\n",
    "            print(\"\\t{}\\t{}\\t{:.3f}\".format(key, value, f1_score))           \n",
    "\n",
    "def do_best_test(X, y, X_test, n_fold, clf):\n",
    "    f1_score = cross_validation(X, np.ravel(y), StratifiedKFold(n_splits=n_fold), clf)\n",
    "    print(\"{:.3f}\".format( f1_score))           \n",
    "            \n",
    "def main(dataset_number, n_folds, test_size, isActive):\n",
    "\n",
    "    ######## Preparing Data ########\n",
    "    \n",
    "    # Load Data\n",
    "    dataset = import_dataset(dataset_number)\n",
    "    \n",
    "    # Split Data\n",
    "    X, y = split_data_on_x_y(dataset)\n",
    "    X_train, X_test, y_train, y_test = split_data_on_training_test(X, y, test_size)\n",
    "    \n",
    "    # Normalize Data\n",
    "    if isActive[0]:\n",
    "        X, X_train, X_test = do_normalization(X, X_train, X_test)\n",
    "\n",
    "\n",
    "\n",
    "    # Cross Validation Test\n",
    "    if isActive[1]:\n",
    "        baggingClassifier = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n",
    "        do_cross_validation_test(X, y, X_test, y_test, baggingClassifier)\n",
    "\n",
    "       \n",
    "        \n",
    "    if isActive[2]:\n",
    "        for i in range(0, 5):\n",
    "            if(i == 0):\n",
    "                params =ParameterGrid({'bootstrap_features': [True, False]})\n",
    "            if(i == 1):\n",
    "                params =ParameterGrid({'bootstrap': [True, False]})\n",
    "            if(i == 2):\n",
    "                params =ParameterGrid({'max_samples': [1, 3, 5, 8, 10]})\n",
    "            if(i == 3):\n",
    "                params =ParameterGrid({'max_features': [1, 2, 5, 7]})\n",
    "            if(i == 4):\n",
    "                params =ParameterGrid({'n_estimators': [1, 5, 10, 15, 20, 40, 50, 100, 300]})\n",
    "            \n",
    "            do_bagging_test(X, y, X_test, n_folds, params)\n",
    "    \n",
    "        \n",
    "    if isActive[3]:\n",
    "        for i in range(0, 3):\n",
    "            if(i == 0):\n",
    "                params =ParameterGrid({'n_estimators': [1, 5, 10, 15, 20, 40, 50, 100, 300]})\n",
    "            if(i == 1):\n",
    "                params =ParameterGrid({'learning_rate': [0.1, 0.5, 1, 2, 5, 10, 20, 50]})\n",
    "            if(i == 2):\n",
    "                params =ParameterGrid({'algorithm': ['SAMME', 'SAMME.R']}) \n",
    "                \n",
    "            do_boosting_test(X, y, X_test, n_folds, params)\n",
    "    \n",
    "    if isActive[4]:\n",
    "        for i in range(0, 3):\n",
    "            if(i == 0):\n",
    "                params =ParameterGrid({'n_estimators': [1, 5, 10, 15, 20, 40, 50, 100, 300]})\n",
    "            if(i == 1):\n",
    "                params =ParameterGrid({'criterion': ['gini', 'entropy']})\n",
    "            if(i == 2):\n",
    "                params =ParameterGrid({'min_samples_leaf': [1, 3, 5, 10, 20, 50]})\n",
    "\n",
    "            do_random_forest_test(X, y, X_test, n_folds, params)\n",
    "            \n",
    "    if isActive[5]:\n",
    "        clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                                bootstrap_features = False, \n",
    "                                bootstrap = True, \n",
    "                                max_samples = 1.0, \n",
    "                                max_features = 1.0, \n",
    "                                n_estimators = 20\n",
    "                               )\n",
    "        do_best_test(X, y, X_test, n_folds, clf)\n",
    "        \n",
    "        clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                n_estimators = 20,\n",
    "                                learning_rate = 2,\n",
    "                                algorithm = 'SAMME.R'\n",
    "                                )\n",
    "        do_best_test(X, y, X_test, n_folds, clf)    \n",
    "                \n",
    "        clf = RandomForestClassifier(n_estimators = 20,\n",
    "                                     criterion = 'gini',\n",
    "                                     min_samples_leaf = 1\n",
    "                                    )\n",
    "        do_best_test(X, y, X_test, n_folds, clf)  \n",
    "        print()\n",
    "        print()\n",
    "    if isActive[6]:\n",
    "            \n",
    "        X, X_train, X_test = do_normalization(X, X_train, X_test)\n",
    "            \n",
    "        clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                                bootstrap_features = False, \n",
    "                                bootstrap = True, \n",
    "                                max_samples = 1.0, \n",
    "                                max_features = 1.0, \n",
    "                                n_estimators = 20\n",
    "                               )\n",
    "        do_best_test(X, y, X_test, n_folds, clf)\n",
    "        \n",
    "        clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                n_estimators = 20,\n",
    "                                learning_rate = 2,\n",
    "                                algorithm = 'SAMME.R'\n",
    "                                )\n",
    "        do_best_test(X, y, X_test, n_folds, clf)    \n",
    "                \n",
    "        clf = RandomForestClassifier(n_estimators = 20,\n",
    "                                     criterion = 'gini',\n",
    "                                     min_samples_leaf = 1\n",
    "                                    )\n",
    "        do_best_test(X, y, X_test, n_folds, clf)  \n",
    "    \n",
    "     \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbootstrap_features\tTrue\t0.666\n",
      "\tbootstrap_features\tFalse\t0.687\n",
      "\tbootstrap\tTrue\t0.685\n",
      "\tbootstrap\tFalse\t0.648\n",
      "\tmax_samples\t1\t0.083\n",
      "\tmax_samples\t3\t0.163\n",
      "\tmax_samples\t5\t0.233\n",
      "\tmax_samples\t8\t0.280\n",
      "\tmax_samples\t10\t0.335\n",
      "\tmax_features\t1\t0.350\n",
      "\tmax_features\t2\t0.457\n",
      "\tmax_features\t5\t0.603\n",
      "\tmax_features\t7\t0.677\n",
      "\tn_estimators\t1\t0.620\n",
      "\tn_estimators\t5\t0.600\n",
      "\tn_estimators\t10\t0.634\n",
      "\tn_estimators\t15\t0.700\n",
      "\tn_estimators\t20\t0.723\n",
      "\tn_estimators\t40\t0.678\n",
      "\tn_estimators\t50\t0.708\n",
      "\tn_estimators\t100\t0.706\n",
      "\tn_estimators\t300\t0.709\n",
      "\n",
      "\tbootstrap_features\tTrue\t0.962\n",
      "\tbootstrap_features\tFalse\t0.929\n",
      "\tbootstrap\tTrue\t0.952\n",
      "\tbootstrap\tFalse\t0.911\n",
      "\tmax_samples\t1\t0.157\n",
      "\tmax_samples\t3\t0.565\n",
      "\tmax_samples\t5\t0.769\n",
      "\tmax_samples\t8\t0.856\n",
      "\tmax_samples\t10\t0.892\n",
      "\tmax_features\t1\t0.753\n",
      "\tmax_features\t2\t0.944\n",
      "\tmax_features\t5\t0.946\n",
      "\tmax_features\t7\t0.962\n",
      "\tn_estimators\t1\t0.902\n",
      "\tn_estimators\t5\t0.912\n",
      "\tn_estimators\t10\t0.963\n",
      "\tn_estimators\t15\t0.962\n",
      "\tn_estimators\t20\t0.951\n",
      "\tn_estimators\t40\t0.957\n",
      "\tn_estimators\t50\t0.962\n",
      "\tn_estimators\t100\t0.962\n",
      "\tn_estimators\t300\t0.962\n",
      "\n",
      "\tbootstrap_features\tTrue\t0.678\n",
      "\tbootstrap_features\tFalse\t0.705\n",
      "\tbootstrap\tTrue\t0.712\n",
      "\tbootstrap\tFalse\t0.659\n",
      "\tmax_samples\t1\t0.367\n",
      "\tmax_samples\t3\t0.488\n",
      "\tmax_samples\t5\t0.595\n",
      "\tmax_samples\t8\t0.621\n",
      "\tmax_samples\t10\t0.616\n",
      "\tmax_features\t1\t0.564\n",
      "\tmax_features\t2\t0.606\n",
      "\tmax_features\t5\t0.682\n",
      "\tmax_features\t7\t0.710\n",
      "\tn_estimators\t1\t0.658\n",
      "\tn_estimators\t5\t0.730\n",
      "\tn_estimators\t10\t0.700\n",
      "\tn_estimators\t15\t0.708\n",
      "\tn_estimators\t20\t0.721\n",
      "\tn_estimators\t40\t0.733\n",
      "\tn_estimators\t50\t0.719\n",
      "\tn_estimators\t100\t0.743\n",
      "\tn_estimators\t300\t0.750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(2, 5, 0.2, [0, 0, 1, 0, 0, 0, 0])\n",
    "main(3, 5, 0.2, [0, 0, 1, 0, 0, 0, 0])\n",
    "main(4, 5, 0.2, [0, 0, 1, 0, 0, 0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
